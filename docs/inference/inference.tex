\documentclass[12pt]{article}

\usepackage{natbib}
\usepackage{apalike}
\usepackage[hypertexnames=false,colorlinks=true,breaklinks]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1.5cm]{geometry}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\usepackage{subcaption}
\usepackage{endfloat}

\newtheorem{claim}{Claim}

\title{Inference in Linear Dynamical Systems}
\author{Joaquin Rapela\thanks{j.rapela@ucl.ac.uk}}

\begin{document}

\maketitle

\section{Linear dynamical systems (LDS) model}
\label{sec:ldsModel}

\begin{alignat*}{4}
    \mathbf{x}_n &= A \mathbf{x}_{n-1}+\mathbf{w}_n\quad && \mathbf{w}_n\sim
    N(\mathbf{w}_n|\mathbf{0}, Q)\quad && \mathbf{x}_n\in\mathbb{R}^m&&\\
    \mathbf{y}_n &= C \mathbf{x}_{n}+\mathbf{v}_n\quad && \mathbf{v}_n\sim
    N(\mathbf{v}_n|\mathbf{0}, R)\quad && \mathbf{y}_n\in\mathbb{R}^n&&\quad n=1\ldots N\\
    \mathbf{x}_0 &\sim N(\mathbf{w}_n|\mathbf{m}_0, V_0) && &&
\end{alignat*}

\begin{figure}[h]
\begin{center}
    \includegraphics[width=5in]{figures/ldsGraphicalModel.pdf}
    \caption{Graphical model for linear dynamical systems}
    \label{fig:ldsGraphicalModel}
\end{center}
\end{figure}

\section{Inference Problems}
\label{sec:inferenceProblems}

\begin{description}
    \item[Prediction]
        \begin{align}
            P(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})=N(\mathbf{x}_n|\mathbf{x}_{n|n-1},P_{n|n-1})\label{eq:prediction}
        \end{align}
    \item[Filtering]
        \begin{align}
            P(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n})=N(\mathbf{x}_n|\mathbf{x}_{n|n},P_{n|n})\label{eq:filtering}
        \end{align}
    \item[Smoothing]
        \begin{align}
            P(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{N})=N(\mathbf{x}_n|\mathbf{x}_{n|N},P_{n|N})\label{eq:smoothing}
        \end{align}
\end{description}

\section{Kalman Filter}

The Kalman filter algorithm addresses the prediction (Eq.~\ref{eq:prediction})
and filtering (Eq.~\ref{eq:filtering}) inference problems.
%
It is an iterative algorithm, which alternates between computing the mean and
covariance of the prediction distribution and computing the mean and covariance
of the filtering distribution.

\begin{alignat*}{2}
    \mathbf{x}_{0|0}&=\mathbf{m}_0&&\text{init filtered mean}\\
    P_{0|0}&=V_0&&\text{init filtered covariance}\\
    \mathbf{x}_{n+1|n}&=A\mathbf{x}_{n|n}\quad&&\text{prediction mean}\\
    P_{n+1|n}&=AP_{n|n}A^\intercal+Q\quad&&\text{prediction covariance}\\
    \mathbf{y}_{n|n-1}&=C\mathbf{x}_{n|n-1}\quad&&\text{predicted observation}\\
    \tilde{\mathbf{y}}_n&=\textcolor{red}{\mathbf{y}_n}-\mathbf{y}_{n|n-1}\quad&&\text{residual}\\
    S_n&=CP_{n|n-1}C^\intercal+R\quad&&\text{residual covariance}\\
    \mathbf{x}_{n|n}&=\mathbf{x}_{n|n-1}+K_n\tilde{\mathbf{y}}_n\quad&&\text{filtering mean}\\
    K_n&=P_{n|n-1}C^\intercal S_n^{-1}&&\text{Kalman gain}\\
    P_{n|n}&=(I_M-K_nC)P_{n|n-1}&&\text{filtering covariance}
\end{alignat*}

Inference of predicted and filtered means and covariances proceeds in a forward
fashion, inferring the prediction and filtering distributions from the first to
the last state, as shown in the next figure.

\begin{figure}[h]
\begin{center}
    \includegraphics[width=5in]{figures/kfAlternations.pdf}
    \caption{Order of calculation of the prediction and filtering distributions
    in the Kalman filtering algorithm.}
    \label{fig:kfAlternations.}
\end{center}
\end{figure}

\section{Kalman Smoother}

The Kalman smoother algorithm addresses the smoothing (Eq.~\ref{eq:smoothing})
inference problem.

\begin{alignat*}{2}
    x_{n|N}&=x_{n|n}+C_n(x_{n+1|N}-x_{n+1|n})\quad&&\text{smoothed mean}\\
    P_{n|N}&=P_{n|n}+C_n(P_{n+1|N}-P_{n+1|n})C_n^\intercal\quad&&\text{smoothed covariance}\\
    C_n&=P_{n|n}A^\intercal P_{n+1|n}^{-1}&&
\end{alignat*}

Inference of smoothed mean and covariances proceeds in a backward fashion:
$x_{N|N}, P_{N|N}\rightarrow x_{N-1|N}, P_{N-1|N}\rightarrow x_{N-2|N},
P_{N-2|N}\rightarrow\ldots\rightarrow  x_{1|N}, P_{1|N}$. The initial mean and
covariances (i.e., $x_{N|N}, P_{N|N}$) are initialized from the last step of
the Kalman filter.

\section{Derivation of Kalman filter equations}

\begin{claim}
    $\mathbf{x}_{n|n-1}=A\mathbf{x}_{n-1|n-1}$
    \label{claim:predictionMean}
\end{claim}

\begin{proof}
    \begin{align}
        \mathbf{x}_{n|n-1}&=E\{\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\nonumber\\
                          &=E\{A\mathbf{x}_{n-1}+\mathbf{w}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\label{eq:c1l2}\\
                          &=AE\{\mathbf{x}_{n-1}|\mathbf{y}_ 1,\ldots,\mathbf{y}_{n-1}\}+E\{\mathbf{w}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\label{eq:c1l3}\\
                          &=A\mathbf{x}_{n-1|n-1}+E\{\mathbf{w}_n\}\label{eq:c1l4}\\
                          &=A\mathbf{x}_{n-1|n-1\label{eq:c1l5}}
    \end{align}
\end{proof}

Notes:

\begin{itemize}

    \item Eq.~\ref{eq:c1l2} arises from the state equation of the LDS model in
        Section~\ref{sec:ldsModel},
    \item Eq.~\ref{eq:c1l3} holds because the expectation distributes over sums
    \item Eq.~\ref{eq:c1l4} uses the definition of $\mathbf{x}_{n-1|n-1}$ and
        the fact that the state noise, $\mathbf{w}_n$, is independent of previous observations.
    \item Eq.~\ref{eq:c1l5} follows due to the zero mean of $\mathbf{w}_n$.
\end{itemize}

\begin{claim}
    $P_{n|n-1}=AP_{n-1|n-1}A^\intercal+Q$
    \label{claim:predictionCov}
\end{claim}

\begin{proof}
    \begin{align}
        P_{n|n-1}=&\text{Cov}\{\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\nonumber\\
                 =&\text{Cov}\{A\mathbf{x}_{n-1}+\mathbf{w}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\label{eq:c2l2}\\
                 =&\text{Cov}\{A\mathbf{x}_{n-1}|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}+\text{Cov}\{\mathbf{w}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\label{eq:c2l3}\\
                 =&A\ \text{Cov}\{\mathbf{x}_{n-1}|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\ A^\intercal+\text{Cov}\{\mathbf{w}_n\}\label{eq:c2l4}\\
                 =&A\ P_{n-1|n-1}\ A^\intercal+Q\label{eq:c2l5}
    \end{align}
\end{proof}
Notes:

\begin{enumerate}
    \item Eq.~\ref{eq:c2l2} used the state equation of the LDS model in
        Section~\ref{sec:ldsModel},
    \item Eq.~\ref{eq:c2l3} is true because $\mathbf{w}_n$ is independent from
        $\mathbf{x}_{n-1}$,
    \item Eq.~\ref{eq:c2l4} holds by the property
        $\text{Cov}\{A\mathbf{x}\}=A\ \text{Cov}\{\mathbf{x}\}' A^\intercal$ and
        because $\mathbf{w}_n$ is independent of previous observations.
    \item Eq.~\ref{eq:c2l4} applied the definitions of $P_{n-1|n-1}$ and $Q$.
\end{enumerate}

\begin{claim}
    $\mathbf{x}_{n|n}=\mathbf{x}_{n|n-1}+K_n\tilde{\mathbf{y}}_n$ and
    $P_{n|n}=(I-K_nC)P_{n|n-1}$.
    \label{claim:filteringMeanAndCov}
\end{claim}

\begin{proof}

    Define the random variables
    $\mathbf{x}=\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}$ and
    $\mathbf{y}=\mathbf{y}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}$. Then
    $\mathbf{x}|\mathbf{y}=\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_n$ and
    the mean and covariance that we want to find, $\mathbf{x}_{n|n}$ and
    $P_{n|n}$, are those of $\mathbf{x}|\mathbf{y}$.  Thus, we want to compute
    the mean, $\mu_{\mathbf{x}|\mathbf{y}}=\mathbf{x}_{n|n}$, and covariance,
    $\Sigma_{\mathbf{x}|\mathbf{y}}=P_{n|n}$, of $\mathbf{x}|\mathbf{y}$.

    Because $\mathbf{x}_n$ and $\mathbf{y}_n$ are jointly Gaussian, then
    $\mathbf{x}$ and $\mathbf{y}$ are also jointly Gaussian. Then, $\mu_{\mathbf{x}|\mathbf{y}}$
    and $\Sigma_{\mathbf{x}|\mathbf{y}}$ are \citep[][Chapter 2]{bishop06}

    \begin{align}
        \mu_{\mathbf{x}|\mathbf{y}}&=\mu_{\mathbf{x}} +
        \Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}(\mathbf{y}_n-\mu_{\mathbf{y}})\label{eq:muxgy}\\
        \Sigma_{\mathbf{x}|\mathbf{y}}&=\Sigma_{\mathbf{x}\mathbf{x}}-\Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}\Sigma_{\mathbf{y}\mathbf{x}}\label{eq:sigmaxgy}
    \end{align}

    Thus, to compute $\mu_{\mathbf{x}|\mathbf{y}}$ and $\Sigma_{\mathbf{x}|\mathbf{y}}$ we need to calculate
    $\mu_{\mathbf{x}}$, $\mu_{\mathbf{y}}$, $\Sigma_{\mathbf{x}\mathbf{x}}$, $\Sigma_{\mathbf{x}\mathbf{y}}$ and $\Sigma_{\mathbf{y}\mathbf{y}}$.

    \rule{2in}{0.4pt}

    \begin{align}
        \mu_{\mathbf{x}}=E\{\mathbf{x}\}=E\{\mathbf{x}_n|\mathbf{y}_1,\ldots\mathbf{y}_{n-1}\}=\mathbf{x}_{n|n-1}\label{eq:mux}
    \end{align}

    \rule{2in}{0.4pt}

    \begin{align}
        \mu_{\mathbf{y}}&=E\{\mathbf{y}\}=E\{\mathbf{y}_n|\mathbf{y}_1,\ldots\mathbf{y}_{n-1}\}=E\{C\mathbf{x}_n+\mathbf{v}_n|\mathbf{y}_1,\ldots\mathbf{y}_{n-1}\}=\nonumber\\
             &=CE\{\mathbf{x}_n|\mathbf{y}_1,\ldots\mathbf{y}_{n-1}\}+E\{\mathbf{v}_n|\mathbf{y}_1,\ldots\mathbf{y}_{n-1}\}=C\mathbf{x}_{n|n-1}+E\{\mathbf{v}_n\}=C\mathbf{x}_{n|n-1}=\mathbf{y}_{n|n-1}\label{eq:muy}
    \end{align}

    Notes:

    \begin{itemize}
        \item The penultimate equality in Eq.~\ref{eq:muy} uses the definition
            of $\mathbf{x}_{n|n-1}$ and the fact that $\mathbf{v}_n$ is
            independent of previous observations.

        \item The last equality in Eq.~\ref{eq:muy} holds because the mean of
            $\mathbf{v}_n$ is zero.
            Claim~\ref{claim:predictionMean}.
    \end{itemize}

    \rule{2in}{0.4pt}

    \begin{align}
        \Sigma_{\mathbf{y}\mathbf{y}}&=\text{Cov}(\mathbf{y}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\nonumber\\
                                     &=\text{Cov}(C\mathbf{x}_n+\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\nonumber\\
                                     &=\text{Cov}(C\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})+\text{Cov}(\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\nonumber\\
                                     &=C\ \text{Cov}(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\ C^\intercal+\text{Cov}(\mathbf{v}_n)\label{eq:c3Sigmayyl4}\\
                                     &=CP_{n|n-1}C^\intercal+R\label{eq:sigmayy}
    \end{align}

    \rule{2in}{0.4pt}

Notes:

\begin{itemize}

    \item As in Eq.~\ref{eq:c2l3}, Eq.~\ref{eq:c3Sigmayyl4} holds by the
        property
        $\text{Cov}\{A\mathbf{x}\}=A\ \text{Cov}\{\mathbf{x}\}\ A^\intercal$ and
        because $\mathbf{v}_n$ is independent of previous observations.

\end{itemize}

    \rule{2in}{0.4pt}

    \begin{align}
        \Sigma_{\mathbf{x}\mathbf{y}}&=\text{cCov}(\mathbf{x}_n,\mathbf{y}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\nonumber\\
                                     &=\text{cCov}(\mathbf{x}_n,C\mathbf{x}_n+\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\nonumber\\
                                     &=\text{cCov}(\mathbf{x}_n,C\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})+\text{cCov}(\mathbf{x}_n,\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})\label{eq:c3Sigmaxyl3}\\
                                     &=\text{Cov}(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})C^\intercal+0\label{eq:c3Sigmaxyl4}\\
                                     &=P_{n|n-1}C^\intercal\label{eq:sigmaxy}
    \end{align}

    Notes:

    \begin{itemize}
        \item the first term in Eq.~\ref{eq:c3Sigmaxyl4} arises from the first
            term of Eq.~\ref{eq:c3Sigmaxyl3} since

            \begin{align*}
                \text{cCov}(\mathbf{x}_n,C\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})&=E\{(\mathbf{x}_n-\mu_\mathbf{x})(C\mathbf{x}_n-C\mu_{\mathbf{x}})^\intercal|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\\
                                                                                            &=E\{(\mathbf{x}_n-\mu_\mathbf{x})(\mathbf{x}_n-\mu_{\mathbf{x}})^\intercal C^\intercal|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\\
                                                                                            &=E\{(\mathbf{x}_n-\mu_\mathbf{x})(\mathbf{x}_n-\mu_{\mathbf{x}})^\intercal|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}C^\intercal\\
                                                                                            &=\text{Cov}(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})C^\intercal
            \end{align*}
        \item the second term in Eq.~\ref{eq:c3Sigmaxyl4} arises from the
            second
            term of Eq.~\ref{eq:c3Sigmaxyl3} since

            \begin{align*}
                \text{cCov}(\mathbf{x}_n,\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})&=E\{(\mathbf{x}_n-\mathbf{x}_{n|n-1})\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\\
                                                                                           &=E\{(\mathbf{x}_n-\mathbf{x}_{n|n-1})|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}E\{\mathbf{v}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}\\
                                                                                           &=E\{(\mathbf{x}_n-\mathbf{x}_{n|n-1})|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}E\{\mathbf{v}_n\}\\
                                                                                           &=E\{(\mathbf{x}_n-\mathbf{x}_{n|n-1})|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1}\}0\\
                                                                                           &=0
            \end{align*}

            the second line follows from the first one because $\mathbf{v}_n$
            is independent of $\mathbf{x}_n$, and the third line follows from
            the second one because $\mathbf{v}_n$ is independent of previous
            observations.

    \end{itemize}
    \rule{2in}{0.4pt}

    \begin{align}
        \Sigma_{\mathbf{x}\mathbf{x}}&=\text{Cov}(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})=P_{n|n-1}\label{eq:sigmaxx}
    \end{align}

    \rule{2in}{0.4pt}

    Having calculated $\mu_{\mathbf{x}}$, $\mu_{\mathbf{y}}$,
    $\Sigma_{\mathbf{x}\mathbf{x}}$, $\Sigma_{\mathbf{x}\mathbf{y}}$ and
    $\Sigma_{\mathbf{y}\mathbf{y}}$ we now use Eqs.~\ref{eq:muxgy},
    \ref{eq:sigmaxgy}, \ref{eq:mux},  \ref{eq:muy},  \ref{eq:sigmayy},
    \ref{eq:sigmaxy}, and \ref{eq:sigmaxx} to  
    obtain $\mathbf{x}_{n|n}$ and $P_{n|n}$.

    \begin{align*}
        \mathbf{x}_{n|n}=\mu_{\mathbf{x}|\mathbf{y}}&=\mu_{\mathbf{x}} + \Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}(\mathbf{y}_n-\mu_{\mathbf{y}})\\
                                                    &=\mathbf{x}_{n|n-1}+P_{n|n-1}C^\intercal S_n^{-1}(\mathbf{y}_n-\mathbf{y}_{n|n-1})\\
                                                    &=\mathbf{x}_{n|n-1}+K_n\tilde{\mathbf{y}_n}\\
        P_{n|n}=\Sigma_{\mathbf{x}|\mathbf{y}}&=\Sigma_{\mathbf{x}\mathbf{x}}-\Sigma_{\mathbf{x}\mathbf{y}}\Sigma_{\mathbf{y}\mathbf{y}}^{-1}\Sigma_{\mathbf{y}\mathbf{x}}\\
                                              &=P_{n|n-1}-P_{n|n-1}C^\intercal S_n^{-1}CP_{n|n-1}\\
                                              &=(I-P_{n|n-1}C^\intercal S_n^{-1}C)P_{n|n-1}\\
                                              &=(I-K_nC)P_{n|n-1}
    \end{align*}

\end{proof}

\section{Joint normality of the states and observations in the LDS}

\begin{claim}
    The state and observation random variables of an LDS $\{\mathbf{x}_0,
    \mathbf{x}_1, \ldots, \mathbf{x}_N, \mathbf{y}_1, \ldots, \mathbf{y}_N\}$
    are jointly normal.
\end{claim}

\begin{proof}

    Note that a set of real random variables $\mathcal{Z}=\{z_1,\ldots,z_N\}$ is
    jointly normal if and only if their joint probability distribution is a
    multivariate Normal distribution, if and only if the logarithm of this
    joint probability distribution is a quadratic function of the random
    variables in $\mathcal{Z}$ (i.e., $\ln P(z_1,\ldots,z_N)=k+\sum_{i=1}^N
    k_1(i)z_i+\sum_{i=1}^N\sum_{j=1}^Nk_2(i,j)z_iz_j$, with $k_1(i)$ and
    $k_2(i,j)$ real numbers).

    Thus, to prove this claim it suffice to show that property $P_n$:\ ``$\log
    P(\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_n, \mathbf{y}_1, \ldots,
    \mathbf{y}_n)$  is a quadratic function of the components of
    $\{\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_n, \mathbf{y}_1, \ldots,
    \mathbf{y}_n\}$'' holds for any positive integer n. We show this by
    induction.

    \begin{description}
        \item[$P_1$]:
            \begin{align}
                \ln P(\mathbf{x}_0, \mathbf{x}_1, \mathbf{y}_1)=&\ln P(\mathbf{y}_1|\mathbf{x}_1)\ln P(\mathbf{x}_1|\mathbf{x}_0)\ln P(\mathbf{x}_0)\nonumber\\
                                                               =&K-\frac{1}{2}(\mathbf{y}_1-C\mathbf{x}_1)^\intercal R^{-1}(\mathbf{y}_1-C\mathbf{x}_1)\nonumber\\
                                                                &-\frac{1}{2}(\mathbf{x}_1-A\mathbf{x}_0)^\intercal Q^{-1}(\mathbf{x}_1-A\mathbf{x}_0)\nonumber\\
                                                                &-\frac{1}{2}(\mathbf{x}_0-\mathbf{m}_0)^\intercal Q^{-1}(\mathbf{x}_0-\mathbf{m}_0)\label{eq:p1}
            \end{align}

            $P_1$ follows from the observation that the components of
            $\mathbf{x}_0$, $\mathbf{x}_1$ and $\mathbf{y_1}$ are combined
            quadratically in Eq.~\ref{eq:p1}.

        \item[$P_n\rightarrow P_{n+1}$]:
            \begin{align}
                \ln P(\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_n, \mathbf{x}_{n+1},\mathbf{y}_1, \ldots, \mathbf{y}_{n}, \mathbf{y}_{n+1})=&\ln P(\mathbf{y}_{n+1}|\mathbf{x}_{n+1})+\nonumber\\
                &\ln P(\mathbf{x}_{n+1}|\mathbf{x}_n)+\nonumber\\
                &\ln P(\mathbf{x}_0,\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{y}_1, \ldots, \mathbf{y}_{n})\nonumber\\
                =&K-\frac{1}{2}(\mathbf{y}_{n+1}-C\mathbf{x}_{n+1})^\intercal R^{-1}(\mathbf{y}_{n+1}-C\mathbf{x}_{n+1})\nonumber\\
                 &-\frac{1}{2}(\mathbf{x}_{n+1}-A\mathbf{x}_{n})^\intercal R^{-1}(\mathbf{x}_{n+1}-A\mathbf{x}_{n})\nonumber\\
                 &+\ln P(\mathbf{x}_0,\mathbf{x}_1,\ldots,\mathbf{x}_n,\mathbf{y}_1, \ldots, \mathbf{y}_{n})\label{eq:pn+1}
            \end{align}

            $P_{n+1}$ follows from the observation that the components of
            $\mathbf{x}_n$, $\mathbf{x}_{n+1}$ and $\mathbf{y}_{n+1}$ are combined
            quadratically in the first two lines of Eq.~\ref{eq:pn+1} and, by
            the inductive hypothesis $P_n$, the elements of
            $\mathbf{x}_0,\ldots,\mathbf{x}_n\mathbf{y}_1,\ldots\mathbf{y}_n$
            are combined quadratically in the last line of Eq.~\ref{eq:pn+1}.

    \end{description}

\end{proof}

\section{Evaluation}

\subsection{Simulations}

We compare the accuracy of the Kalman filter and smoother with that of the
method of finite differences, to infer velocities and accelerations of a
simulated object following the dynamics of the Discrete Wiener Process
Acceleration (DWPA)
model\footnote{\url{https://github.com/joacorapela/lds_python/blob/master/docs/tracking/tracking.pdf}}.  We used the following parameters in the
simulations:

% \begin{table}[h!]
    % \centering
\begin{center}
    \begin{tabular}{|l|c|}\hline
        \multicolumn{1}{|c|}{\emph{Name}} & \multicolumn{1}{|c|}{\emph{Value}} \\\hline\hline
        $\mathbf{x}_0$                        & $[0, 0]$\\\hline
        $V_0$                                 & diag([0.001, 0.001])\\\hline
        $\gamma=\gamma_1=\gamma_2$            & 1.0\\\hline
        $\sigma=\sigma_1=\sigma_2$            & varied\\\hline
    \end{tabular}
\end{center}
    % \caption{Simulation parameters}
% \end{table}

\noindent We simulated $N=10,000$ samples with a step size $dt=0.001$.

\subsubsection{Lower noise ($\sigma=\num{1e-10}$)}

We simulated measurements from the two-dimensional DWPA model with a standard
deviation for the noise of the observations set to $\sigma=\num{1e-10}$.
Figure~\ref{fig:simulations_low_noise} shows the state positions in blue and
the noise-corrupted measurements in black.

\begin{figure}

    \centering
    \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/28152456_simulation_pos.html}{\includegraphics[width=5in]{../../figures/28152456_simulation_pos.png}}

    \caption{Noise corrupted measurements (black) and state positions
    (blue) simulated with low noise (standard deviation $\sigma=\num{1e-10}$)
    using a two-dimensional DWPA model. The noise was so low that the
    differences between measurements and state positions cannot be appreciated
    visually.  The mean-squared error (MSE) between measurements and state
    positions is indcated in the title.
    Click on the image to view its interactive version.}

        \label{fig:simulations_low_noise}

\end{figure}

We next inferred velocities and accelerations from the simulated measurements
(Figure~\ref{fig:vel_acc_low_noise}). For inference we used the Kalman filter,
Kalman smoother and the finite difference method. For these low-noise
simulations, all velocity and acceleration estimates were very accurate.

\begin{figure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/69044958_vel_smoothed.html}{\includegraphics[width=5in]{../../figures/69044958_vel_smoothed.png}}
        \caption{Velocity}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/69044958_acc_smoothed.html}{\includegraphics[width=5in]{../../figures/69044958_acc_smoothed.png}}
        \caption{Acceleration}
    \end{subfigure}

    \caption{Estimated velocities (a) and accelerations (b) from low-noise
    simulations.  Estimates were obtained using the finite differences method,
    the Kalman filter and the Kalman smoother. Velocity and Acceleration
    estimates using all methods were very accurate.
    Click on the image to view its interactive version.}

    \label{fig:vel_acc_low_noise}

\end{figure}

\subsubsection{Medium noise ($\sigma=\num{1e-3}$)}

We simulated measurements from the two-dimensional DWPA model with a standard
deviation for the noise of the observations set to $\sigma=\num{1e-3}$.
Figure~\ref{fig:simulations_medium_noise} shows the state positions in blue and
the noise-corrupted measurements in black.

\begin{figure}

    \centering
    \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/23897501_simulation_pos.html}{\includegraphics[width=5in]{../../figures/23897501_simulation_pos.png}}

    \caption{Noise corrupted measurements (black) and state positions (blue)
    simulated with medium noise (standard deviation $\sigma=\num{1e-3}$) using
    a two-dimensional DWPA model.  The noise was still so low that the
    differences between measurements and state positions cannot be appreciated
    visually.  The MSE between measurements and state positions is indicated in
    the title. Click on the image to view its interactive version.}

    \label{fig:simulations_medium_noise}

\end{figure}

We next inferred velocities and accelerations from the simulated measurements
(Figure~\ref{fig:vel_acc_medium_noise}). For these medium-noise simulations,
all velocity estimates were accurate. The Kalman filter and smoother estimates
of acceleration were also accurate, but the finite difference estimates of
acceleration were not.

\begin{figure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/01448032_vel_smoothed.html}{\includegraphics[width=5in]{../../figures/01448032_vel_smoothed.png}}
        \caption{Velocity}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/01448032_acc_smoothed.html}{\includegraphics[width=5in]{../../figures/01448032_acc_smoothed.png}}
        \caption{Acceleration}
    \end{subfigure}

    \caption{Estimated velocities (a) and accelerations (b) from medium-noise
    simulations. Estimates were obtained using the finite differences method,
    the Kalman filter and Kalman smoother.  Estimates of velocity by all
    methods were accurate. Estimates of accelerations by the Kalman filter and
    smoother were also accurate, but not those by the finite difference method.
    Click on the image to view its interactive version.}

    \label{fig:vel_acc_medium_noise}

\end{figure}

\subsubsection{Higher noise ($\sigma=\num{1e-1}$)}

We simulated measurements from the two-dimensional DWPA model with a standard
deviation for the noise of the observations set to $\sigma=\num{1e-1}$.
Figure~\ref{fig:simulations_high_noise} shows the state positions in blue and
the noise-corrupted measurements in black.

\begin{figure}

    \centering
    \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/01112777_simulation_pos.html}{\includegraphics[width=5in]{../../figures/01112777_simulation_pos.png}}

    \caption{Noise corrupted measurements (black) and state positions (blue)
    simulated with high noise (standard deviation $\sigma=\num{1e-1}$) using a
    two-dimensional DWPA model. The noise can now be appreciated visually.  The
    MSE between measurements and state positions is indicated in the title.
    Click on the image to view its interactive version.}

    \label{fig:simulations_high_noise}

\end{figure}

We next inferred velocities and accelerations from the simulated measurements
(Figure~\ref{fig:vel_acc_high_noise}).  For these high-noise simulations,
velocity and acceleration estimates by the Kalman filter and smoother were
accurate, but those from the finite difference method were not.

\begin{figure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/58788639_vel_smoothed.html}{\includegraphics[width=5in]{../../figures/58788639_vel_smoothed.png}}
        \caption{Velocity}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/58788639_acc_smoothed.html}{\includegraphics[width=5in]{../../figures/58788639_acc_smoothed.png}}
        \caption{Acceleration}
    \end{subfigure}

    \caption{Estimated velocities (a) and accelerations (b) from high-noise
    simulations.  Estimates were obtained using the finite differences method,
    the Kalman filter and the Kalman smoother. Velocity and Acceleration
    estimates by the Kalman filter and smoother were accurate, but not those by
    the finite differences methods. Click on the image to view its interactive
    version.}

    \label{fig:vel_acc_high_noise}

\end{figure}

\subsubsection{Conclusions}

It is remarkable that the finite difference method breaks down when adding very
little noise to the true measurements. As we increased the amount of noise,
this break down happened earlier for accelerations than for velocities.
%
The Kalman filter and smoother were robust to the amount of noise considered
here, both for the estimation of velocities and accelerations.

\subsection{Foraging mouse}

Figure~\ref{fig:foraging} shows the measured and Kalman filtered and smoothed
positions of a mouse foraging in a circular arena.

\begin{figure}
    \begin{center}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_pos.html}{\includegraphics[width=3in]{../../figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_pos.png}}
            \caption{Positions}
        \end{subfigure}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_vel.html}{\includegraphics[width=3in]{../../figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_vel.png}}
            \caption{Velocities}
        \end{subfigure}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_acc.html}{\includegraphics[width=3in]{../../figures/positions_smoothed_session003_start0.00_end15548.27_startPosition0_numPosition10000_acc.png}}
            \caption{Accelerations}
        \end{subfigure}


        \caption{Kalman filtered and smoothed positions, velocities and
        accelerations of a mouse foraging in a circular arena. Click on the
        images to see their interactive versions.}

        \label{fig:foraging}

    \end{center}
\end{figure}

\subsection{Mouse running on a maze}

Figure~\ref{fig:runningMaze} shows the measured and Kalman filtered and smoothed
positions of a mouse running in a maze.

\begin{figure}
    \begin{center}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/smoothed_results_fede_firstSample0_numberOfSamples10000_pos.html}{\includegraphics[width=3in]{../../figures/smoothed_results_fede_firstSample0_numberOfSamples10000_pos.png}}
            \caption{Positions}
        \end{subfigure}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/smoothed_results_fede_firstSample0_numberOfSamples10000_vel.html}{\includegraphics[width=3in]{../../figures/smoothed_results_fede_firstSample0_numberOfSamples10000_vel.png}}
            \caption{Velocities}
        \end{subfigure}

        \begin{subfigure}{\textwidth}
            \centering
            \href{http://www.gatsby.ucl.ac.uk/~rapela/fwg/lds_repo/inference/figures/smoothed_results_fede_firstSample0_numberOfSamples10000_acc.html}{\includegraphics[width=3in]{../../figures/smoothed_results_fede_firstSample0_numberOfSamples10000_acc.png}}
            \caption{Accelerations}
        \end{subfigure}


        \caption{Kalman filtered and smoothed positions, velocities and
        accelerations of a mouse running in a maze. Click on the images to see
        their interactive versions.}

        \label{fig:runningMaze}

    \end{center}
\end{figure}

\bibliographystyle{apalike}
\bibliography{machineLearning}

\end{document}
